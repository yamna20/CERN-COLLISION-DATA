{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yamna20/CERN-COLLISION-DATA/blob/main/batch_gd_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REKre4vg8idY",
        "outputId": "47b7eda6-5ce5-43e1-dcdd-0867aad7b16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta_one1.1600000000000001e-15 , theta_zero3.96e-14 , j3660.0 , iterations0 \n",
            "theta_one2.319999999999976e-15 , theta_zero7.919999999999913e-14 , j3659.999999999843 , iterations1 \n",
            "theta_one3.4799999999999288e-15 , theta_zero1.1879999999999738e-13 , j3659.9999999996858 , iterations2 \n",
            "theta_one4.639999999999858e-15 , theta_zero1.5839999999999476e-13 , j3659.9999999995293 , iterations3 \n",
            "theta_one5.799999999999763e-15 , theta_zero1.9799999999999128e-13 , j3659.9999999993724 , iterations4 \n",
            "theta_one6.9599999999996445e-15 , theta_zero2.3759999999998694e-13 , j3659.999999999215 , iterations5 \n",
            "theta_one8.119999999999502e-15 , theta_zero2.7719999999998174e-13 , j3659.9999999990587 , iterations6 \n",
            "theta_one9.279999999999337e-15 , theta_zero3.1679999999997564e-13 , j3659.9999999989013 , iterations7 \n",
            "theta_one1.0439999999999146e-14 , theta_zero3.5639999999996867e-13 , j3659.999999998745 , iterations8 \n",
            "theta_one1.1599999999998931e-14 , theta_zero3.9599999999996085e-13 , j3659.999999998587 , iterations9 \n"
          ]
        }
      ],
      "source": [
        "#batch gradient descent algorithim.\n",
        "#our predicted or hypothesis function h(x)= theta zero + thetaone*x (where theta zero is y intercept and thetaone is slope)\n",
        "#cost function is j(thetaone,thetazero)= (1/m)*sum(predicted function/h(x) - y)\n",
        "#lets import the required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#defining gradient descent as a function of x and y\n",
        "def gradient_descent(x,y):\n",
        "\n",
        "  theta_zero = 0\n",
        "  theta_one = 0\n",
        "  #numberof iteration we want\n",
        "  iterations = 10\n",
        "#length of our data set\n",
        "  n = len(x)\n",
        "#alpha our learning parameters\n",
        "  alpha = 0.00000000000000001\n",
        "\n",
        "#initiating loop\n",
        "  for i in range(iterations):\n",
        "   h= theta_zero + theta_one*x\n",
        "   J = (1/n)*sum([val**2 for val in (y-h)])\n",
        "   derivative_theta_one = -(2/n)*sum(x*(y-h))\n",
        "   derivative_theta_zero = -(2/n)*sum(y-h)\n",
        "#new values of thetaone and thetazero\n",
        "   theta_one = theta_one - alpha*derivative_theta_one\n",
        "   theta_zero = theta_zero - alpha*derivative_theta_zero\n",
        "   print(\"theta_one{} , theta_zero{} , j{} , iterations{} \".format(theta_zero , theta_one ,J ,i ))\n",
        "x = np.array([10,20,30,40,50])\n",
        "y = np.array([30,50,60,70,80])\n",
        "gradient_descent(x,y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}